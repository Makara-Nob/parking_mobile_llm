{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System on Google Colab (Free Tier)\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system using **free open-source models** on a standard Google Colab T4 instance.\n",
    "\n",
    "### **Pipeline:**\n",
    "1. **Unstructured Data** -> Text Chunks\n",
    "2. **Text Chunks** -> Embeddings (Sentence Transformers)\n",
    "3. **Embeddings** -> Vector DB (ChromaDB)\n",
    "4. **Query** -> LLM (Zephyr-7B-beta, 4-bit quantized) -> Answer\n",
    "\n",
    "**Note**: Make sure your Runtime is set to **GPU** (T4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# - transformers, accelerate, bitsandbytes: For loading the LLM\n",
    "# - langchain*: For the RAG orchestration\n",
    "# - chromadb: Vector database\n",
    "# - sentence-transformers: For embeddings\n",
    "# - unstructrued, pypdf: For loading data\n",
    "!pip install -q -U requests==2.32.4 \"opentelemetry-sdk<1.39.0\" transformers accelerate bitsandbytes langchain langchain-community sentence-transformers chromadb pypdf unstructured networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Using Zephyr 7B Beta (Instruction Tuned Mistral)\n",
    "MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "# Standard lightweight embedding model\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Quantization Config for 4-bit loading (Required for Colab Free Tier T4 GPU)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD LLM ---\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create a generation pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "print(\"LLM Loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA PIPELINE: AUTOMATING UNSTRUCTURED TO STRUCTURED ---\n",
    "\n",
    "# 1. Setup Data Directory\n",
    "DATA_PATH = \"/content/data\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Please upload your files (.txt, .pdf) to {DATA_PATH}\")\n",
    "\n",
    "# Check if directory is empty, create demo file if so\n",
    "if not any(fname.endswith(('.txt', '.pdf')) for fname in os.listdir(DATA_PATH)):\n",
    "    print(\"No Text/PDF data found in folder. Creating a demo file.\")\n",
    "    with open(os.path.join(DATA_PATH, \"demo_info.txt\"), \"w\") as f:\n",
    "        f.write(\"The Neural RAG system was built in 2024. It uses Zephyr-7B as its core brain. \"\n",
    "                \"Data structuring allows unstructured text to be queried efficiently. \"\n",
    "                \"The capital of the Moon is currently unknown, but cheese is a popular theory.\")\n",
    "\n",
    "# 2. Load Documents\n",
    "print(\"Loading documents...\")\n",
    "documents = []\n",
    "# simple loader for text files\n",
    "txt_loader = DirectoryLoader(DATA_PATH, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "documents.extend(txt_loader.load())\n",
    "\n",
    "# loader for PDF files\n",
    "try:\n",
    "    pdf_loader = DirectoryLoader(DATA_PATH, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents.extend(pdf_loader.load())\n",
    "except Exception as e:\n",
    "    print(f\"Could not load PDFs (maybe none exist or missing dependency): {e}\")\n",
    "\n",
    "print(f\"Total Documents Loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split Text (Chunking)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Data broken down into {len(chunks)} structured chunks.\")\n",
    "\n",
    "# 4. Embeddings & Vector Store\n",
    "print(\"Creating Embeddings and Vector Index...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={'device': 'cpu'} # Use CPU for embeddings to save VRAM for the LLM\n",
    ")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"/content/chroma_db\"\n",
    ")\n",
    "print(\"Vector Database Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RETRIEVAL SETUP ---\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Zephyr/Mistral Prompt Template\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "You are a helpful AI assistant. Use the following pieces of context to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}</s>\n",
    "<|user|>\n",
    "{question}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST THE PIPELINE ---\n",
    "def ask_rag(query):\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    print(f\"\\nQuestion: {query}\")\n",
    "    print(\"Answer:\", result['result'])\n",
    "    print(\"\\n[Retrieved Sources]\")\n",
    "    for i, doc in enumerate(result['source_documents']):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        print(f\"{i+1}. {source}: {doc.page_content[:100]}...\")\n",
    "\n",
    "# Ask a question based on your data\n",
    "ask_rag(\"What is this system built with?\")\n",
    "# ask_rag(\"Your own question here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}